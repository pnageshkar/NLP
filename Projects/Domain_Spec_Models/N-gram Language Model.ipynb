{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Workflow\n1. Split the dataset into training and a testing subset. Use the category “title” for the testing set and the categories “comment” and “post” for the training set. The short length of titles will make them good candidates later as seeds for text generation.\n2. \tBuild the matrix of prefix—word frequencies.\n -  \tUse the ngrams function from nltk.utils to generate all n-grams from the corpus\n -  \tSet the following left_pad_symbol = <s> and right_pad_symbol = </s>\n3. \tWrite a text generation function:\n -  \tTakes a bigram as input and generates the next token\n -  \tIteratively slide the prefix over the generated text so that the new prefix includes the most recent token; generates the next token\n -  \tTo generate each next token, sample the list of words associated with the prefix using the probability distribution of the prefix\n -\t    Stop the text generation when a certain number of words have been generated or the latest token is a </s>.\n4.\tWrite a function that can estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences.\n-   \tSplit the sentence into trigrams and use the chain rule to calculate the probability of the sentence as a product of the bigrams—tokens probabilities\n5.\tImplement the perplexity scoring function for a given sentence and for the training corpus.\n6.\tImplement Additive Laplace smoothing to give a non-zero probability to missing prefix—token combinations when calculating perplexity.\n7.\tCalculate the perplexity of the language model on the test set composed of titles.\n8.\tTry to improve the perplexity score of your model by:\n -   \tmodifying the pre-processing phase of the corpus,\n -   \tincreasing or decreasing number of tokens in the model (bi grams, 4-grams, etc.),\n -   \tvarying the delta parameter in the Additive Laplace smoothing step.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## 1. Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/build-dom-spec-models/stackexchange_812k_v2.csv')\ndata.head()","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"   post_id  parent_id  comment_id  \\\n0   291254        NaN    601672.0   \n1   115372        NaN    221284.0   \n2   327356        NaN         NaN   \n3   186923        NaN    355055.0   \n4   433143        NaN         NaN   \n\n                                                text category  \\\n0  The condition makes the gradient unbiased. it ...  comment   \n1                       Yes, that sounds fine to me.  comment   \n2  Consider gaussian variables belonging to a gau...     post   \n3  Thanks S. Catterall. - Integrability I knew th...  comment   \n4               Feature with very few extreme values    title   \n\n                                              tokens  n_tokens  \n0  the condition makes the gradient unbiased . it...        17  \n1                     yes , that sounds fine to me .         8  \n2  consider gaussian variables belonging to a gau...        31  \n3  thanks s . catterall . - integrability i knew ...        30  \n4               feature with very few extreme values         6  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>post_id</th>\n      <th>parent_id</th>\n      <th>comment_id</th>\n      <th>text</th>\n      <th>category</th>\n      <th>tokens</th>\n      <th>n_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>291254</td>\n      <td>NaN</td>\n      <td>601672.0</td>\n      <td>The condition makes the gradient unbiased. it ...</td>\n      <td>comment</td>\n      <td>the condition makes the gradient unbiased . it...</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115372</td>\n      <td>NaN</td>\n      <td>221284.0</td>\n      <td>Yes, that sounds fine to me.</td>\n      <td>comment</td>\n      <td>yes , that sounds fine to me .</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>327356</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Consider gaussian variables belonging to a gau...</td>\n      <td>post</td>\n      <td>consider gaussian variables belonging to a gau...</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>186923</td>\n      <td>NaN</td>\n      <td>355055.0</td>\n      <td>Thanks S. Catterall. - Integrability I knew th...</td>\n      <td>comment</td>\n      <td>thanks s . catterall . - integrability i knew ...</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>433143</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Feature with very few extreme values</td>\n      <td>title</td>\n      <td>feature with very few extreme values</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 2.Split the dataset into training and a testing subset.\nUse the category “title” for the testing set and the categories “comment” and “post” for the training set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check distinct values of category\ndata['category'].value_counts()","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"comment    540587\npost       165377\ntitle       83685\nName: category, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = data[data.category == 'title']","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_test.head()","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = data[data.category != 'title']","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"   post_id  parent_id  comment_id  \\\n0   291254        NaN    601672.0   \n1   115372        NaN    221284.0   \n2   327356        NaN         NaN   \n3   186923        NaN    355055.0   \n5   366261        NaN    688119.0   \n\n                                                text category  \\\n0  The condition makes the gradient unbiased. it ...  comment   \n1                       Yes, that sounds fine to me.  comment   \n2  Consider gaussian variables belonging to a gau...     post   \n3  Thanks S. Catterall. - Integrability I knew th...  comment   \n5  Maybe I'm just a Bayesian at heart A Bayesian ...  comment   \n\n                                              tokens  n_tokens  \n0  the condition makes the gradient unbiased . it...        17  \n1                     yes , that sounds fine to me .         8  \n2  consider gaussian variables belonging to a gau...        31  \n3  thanks s . catterall . - integrability i knew ...        30  \n5  maybe i ' m just a bayesian at heart a bayesia...        87  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>post_id</th>\n      <th>parent_id</th>\n      <th>comment_id</th>\n      <th>text</th>\n      <th>category</th>\n      <th>tokens</th>\n      <th>n_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>291254</td>\n      <td>NaN</td>\n      <td>601672.0</td>\n      <td>The condition makes the gradient unbiased. it ...</td>\n      <td>comment</td>\n      <td>the condition makes the gradient unbiased . it...</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115372</td>\n      <td>NaN</td>\n      <td>221284.0</td>\n      <td>Yes, that sounds fine to me.</td>\n      <td>comment</td>\n      <td>yes , that sounds fine to me .</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>327356</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Consider gaussian variables belonging to a gau...</td>\n      <td>post</td>\n      <td>consider gaussian variables belonging to a gau...</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>186923</td>\n      <td>NaN</td>\n      <td>355055.0</td>\n      <td>Thanks S. Catterall. - Integrability I knew th...</td>\n      <td>comment</td>\n      <td>thanks s . catterall . - integrability i knew ...</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>366261</td>\n      <td>NaN</td>\n      <td>688119.0</td>\n      <td>Maybe I'm just a Bayesian at heart A Bayesian ...</td>\n      <td>comment</td>\n      <td>maybe i ' m just a bayesian at heart a bayesia...</td>\n      <td>87</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":51,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 705964 entries, 0 to 789648\nData columns (total 7 columns):\n #   Column      Non-Null Count   Dtype  \n---  ------      --------------   -----  \n 0   post_id     705964 non-null  int64  \n 1   parent_id   74508 non-null   float64\n 2   comment_id  540587 non-null  float64\n 3   text        705964 non-null  object \n 4   category    705964 non-null  object \n 5   tokens      705964 non-null  object \n 6   n_tokens    705964 non-null  int64  \ndtypes: float64(2), int64(2), object(3)\nmemory usage: 43.1+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":52,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 83685 entries, 4 to 789619\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   post_id     83685 non-null  int64  \n 1   parent_id   0 non-null      float64\n 2   comment_id  0 non-null      float64\n 3   text        83685 non-null  object \n 4   category    83685 non-null  object \n 5   tokens      83685 non-null  object \n 6   n_tokens    83685 non-null  int64  \ndtypes: float64(2), int64(2), object(3)\nmemory usage: 5.1+ MB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 3. Build the matrix of prefix—word frequencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.util import ngrams\nfrom collections import defaultdict , Counter\nimport math","execution_count":53,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create the Vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all tokens in training set\ntrain_tokens = []\nfor txt in df_train['tokens']:\n    for tok in txt.split():\n        train_tokens.append(tok)\n\n\n# create dictionary {token:number of occurances in train set}\ntok_count = Counter(train_tokens)\n\n# remove tokens with less than 10 occurances\ntok_count_f = {key:val for key, val in tok_count.items() if val >= 10}\n\n# The Vocabulary of the stack exchange corpus\nvocab = list(set(tok_count_f.keys()))","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"['dlogllc',\n 'corelation',\n 'argmin',\n 'computation',\n 'donations',\n 'ivv',\n 'coarser',\n 'ph',\n 'mud',\n 'advancing',\n 'damning',\n 'hosmer',\n 'enrolling',\n 'rpg',\n 'they',\n 'grouped',\n 'bender',\n 'rbms',\n 'grades',\n 'yoy',\n 'cval',\n 'layers',\n 'cgpa',\n 'maneuver',\n 'prepend',\n 'merriam',\n 'albedo',\n 'sokal',\n 'keyed',\n 'intake',\n 'angela',\n 'startup',\n 'fluorescence',\n 'onehotencoder',\n 'sqft',\n 'procedures',\n 'kutner',\n 'hexbin',\n 'wx',\n 'flats',\n 'if',\n 'soluble',\n 'drowning',\n 'reasonableness',\n 'hspace',\n 'credited',\n 'detract',\n 'fathers',\n 'scad',\n 'loses',\n 'remarked',\n 'ohe',\n 'herewith',\n 'level',\n 'oscillatory',\n 'damage',\n 'cforest',\n 'fulfills',\n 'krylov',\n 'dots',\n 'rrf',\n 'indqual',\n 'fifty',\n 'hardt',\n 'ranged',\n 'award',\n 'kindly',\n 'haz',\n 'multicolinearity',\n 'broken',\n 'hype',\n 'pysal',\n 'middling',\n 'intr',\n '------------------------------------------------------------------',\n 'preparation',\n 'algorith',\n 'independent',\n 'mix',\n 'gof',\n 'preselected',\n 'investors',\n 'suggesting',\n 'ray',\n 'acne',\n 'tony',\n 'combinations',\n 'broke',\n 'exhaustively',\n 'magical',\n 'rasters',\n 'flushes',\n 'resolve',\n 'euc',\n 'enroll',\n 'preoperative',\n 'query',\n 'encog',\n 'subsampling',\n 'filed',\n 'cues',\n 'coeftest',\n 'notified',\n 'resuls',\n 'lawsuit',\n 'provocative',\n 'naughty',\n 'mandatory',\n 'misspecified',\n 'jamie',\n 'prompting',\n 'tailed',\n 'until',\n 'tones',\n 'universe',\n 'elegantly',\n 'minorities',\n 'ndarray',\n 'presumed',\n 'semester',\n 'shat',\n 'municipality',\n 'auditors',\n 'epidemiol',\n 'probs',\n 'thereafter',\n 'xmat',\n 'rweibull',\n 'un',\n 'commercially',\n 'modified',\n 'decouple',\n 'shortage',\n 'repeat',\n 'mole',\n 'mmarried',\n 'anxious',\n 'apc',\n 'schizophrenia',\n 'safeguards',\n 'neutrals',\n 'extroversion',\n 'invalidate',\n 'finding',\n 'hands',\n 'subcommand',\n 'discriminator',\n 'ant',\n 'gleser',\n 'worth',\n 'mucus',\n 'svmradial',\n 'informative',\n 'sbux',\n 'hyperlinks',\n '√',\n 'compressing',\n 'cockroaches',\n ',,',\n 'bcat',\n 'bunches',\n 'witness',\n 'shuffle',\n 'promoting',\n 'geneticists',\n 'inp',\n 'captured',\n 'living',\n 'awhile',\n 'rebuild',\n 'reconciling',\n 'ohm',\n 'azure',\n 'vast',\n 'analysing',\n 'sticking',\n 'gameplay',\n 'parameterize',\n 'adhered',\n 'wiser',\n '..........',\n 'administration',\n 'dow',\n 'webpage',\n 'imagery',\n 'menten',\n 'syntax',\n 'bicycle',\n 'handbooks',\n 'ftest',\n 'mollusk',\n 'iir',\n 'betting',\n 'mr',\n 'flexsurv',\n 'foreground',\n 'rhyme',\n 'distinctly',\n 'budge',\n 'reputation',\n 'quadr',\n 'brglmfit',\n 'categorically',\n 'exacerbate',\n 'fragmentation',\n 'fractional',\n 'hump',\n 'accustomed',\n 'bms',\n 'pitcher',\n 'tremendously',\n 'getvarcov',\n 'mcq',\n 'thankful',\n 'partners',\n 'arp',\n 'architecture',\n 'meaningfully',\n 'grace',\n 'descendants',\n 'sumproduct',\n 'excursions',\n 'horizon',\n 'fb',\n 'whiteness',\n 'caffe',\n 'elkan',\n 'aridity',\n 'scrolling',\n 'colsums',\n 'gate',\n 'biochemistry',\n 'cultivar',\n 'arose',\n 'sctest',\n 'mach',\n 'dissuade',\n 'nov',\n 'ordering',\n 'municipalities',\n 'perceptual',\n 'hadamard',\n 'yan',\n 'viruses',\n 'macqueen',\n 'spider',\n 'mathematicians',\n 'withdrawn',\n 'profs',\n 'grids',\n 'creatively',\n 'fitdf',\n 'subtract',\n 'canopy',\n 'toxicity',\n 'disk',\n 'pfdr',\n 'unitless',\n 'downward',\n 'disadvantage',\n '?-',\n 'inflows',\n 'organisms',\n 'acf',\n 'arising',\n 'bandits',\n 'numdf',\n 'covariances',\n 'assist',\n 'alle',\n 'inactivity',\n 'maximises',\n 'tan',\n 'kit',\n 'down',\n 'otu',\n 'highorlow',\n 'joy',\n 'lstx',\n 'underlying',\n 'cutoff',\n 'slowness',\n 'verses',\n 'krajjhm',\n 'matrixes',\n 'backpropogation',\n 'promotes',\n 'stringr',\n 'relevent',\n 'dims',\n 'ijk',\n 'overconfident',\n 'metropolitan',\n 'exhaustive',\n 'elevators',\n 'varibales',\n 'reconstruct',\n 'warnings',\n 'wallet',\n 'qbinom',\n 'doubted',\n 'mann',\n 'coulombe',\n 'unc',\n 'anatomical',\n 'gca',\n 'residues',\n 'pitchers',\n 'szekely',\n 'scorecards',\n 'tvec',\n 'rse',\n 'critique',\n 'dollar',\n 'crossvalidate',\n 'travel',\n 'area',\n 'wy',\n 'postulates',\n 'yearvar',\n 'software',\n 'prompted',\n 'redheads',\n 'sgplot',\n 'deb',\n 'nesterov',\n 'ssa',\n 'wednesday',\n 'optimx',\n 'trait',\n 'dpi',\n 'occluded',\n 'curing',\n 'xl',\n 'epa',\n 'penalizer',\n 'minim',\n 'tenure',\n 'calculates',\n 'mo',\n 'randomized',\n 'existential',\n 'totalcost',\n 'irrespectively',\n 'feeders',\n 'surrounds',\n 'perfect',\n 'pythonic',\n 'aspirin',\n 'healthier',\n 'cont',\n 'fiml',\n 'specifically',\n 'touches',\n 'criticism',\n 'developed',\n 'graphics',\n 'ready',\n 'inelegant',\n 'separator',\n 'inefficiencies',\n 'specially',\n 'bioconductor',\n 'ascend',\n 'categorized',\n 'itpc',\n 'support',\n 'prewhiten',\n 'tears',\n 'unix',\n 'attractive',\n 'mackinnon',\n 'kriging',\n 'supersedes',\n 'forwards',\n 'hairy',\n 'hubbard',\n 'aux',\n 'rerun',\n 'cvpartition',\n 'dive',\n 'rectify',\n 'respl',\n 'hugely',\n 'unusual',\n 'theano',\n 'packets',\n 'counties',\n 'phrasing',\n 'miguel',\n 'tenuous',\n 'pearsonian',\n 'animal',\n 'extraversion',\n 'managerial',\n 'tsclust',\n 'variously',\n 'summarised',\n 'unlike',\n 'olejnik',\n 'spurious',\n 'statical',\n 'fortunately',\n 'revol',\n 'medoid',\n 'deteriorate',\n 'depv',\n 'persuasive',\n 'celebrity',\n 'treatedi',\n 'defective',\n 'cumulant',\n 'paraphrased',\n 'whack',\n 'illuminated',\n 'vanished',\n 'trimming',\n 'lsmean',\n 'fscore',\n 'cpue',\n \"...'?\",\n 'harder',\n 'translated',\n 'downvoter',\n 'lms',\n 'understand',\n 'catreg',\n 'topic',\n 'psychometry',\n 'shadish',\n 'deterministic',\n 'interestid',\n 'rosenthal',\n 'organizations',\n 'peaking',\n 'reframing',\n 'echo',\n 'intrinsic',\n 'wm',\n '-------------------------------------------------------------',\n 'rao',\n 'mfrow',\n '!,',\n 'drinker',\n 'oxymoron',\n 'hanna',\n 'catchy',\n 'cfg',\n 'qk',\n 'strains',\n 'lp',\n 'ascii',\n 'grill',\n 'significance',\n 'sigmoids',\n 'opaque',\n 'organizational',\n 'lipschitz',\n 'koren',\n 'understandings',\n 'lots',\n 'offsets',\n 'approximately',\n 'no',\n 'groupe',\n 'fmcd',\n 'bodied',\n 'lopez',\n 'annotated',\n 'invisible',\n 'therefore',\n 'officially',\n 'dglength',\n 'mphw',\n 'improbable',\n 'dominate',\n 'unneeded',\n 'indel',\n 'popping',\n 'leveraging',\n 'divisor',\n 'topologically',\n 'interleaved',\n 'prf',\n 'nursing',\n 'fish',\n 'diminishes',\n 'defect',\n 'lol',\n 'cl',\n 'combat',\n 'nuances',\n 'xgbtree',\n 'hydro',\n 'normalmixem',\n 'blood',\n 'automagically',\n 'psus',\n 'bears',\n 'hot',\n 'qustion',\n 'pollster',\n 'snd',\n 'latest',\n 'radiation',\n 'extinction',\n 'categorize',\n 'refining',\n 'articulated',\n 'correspondences',\n 'hong',\n 'varimax',\n 'reprots',\n 'bulls',\n 'phil',\n 'audit',\n 'unclassified',\n 'perceptron',\n 'everybody',\n 'malfunction',\n 'favours',\n 'mccullough',\n 'ts',\n 'standart',\n 'emit',\n 'booths',\n 'parametrized',\n 'une',\n 'wendy',\n 'mercer',\n 'juror',\n 'vk',\n 'hypervolume',\n 'eigensolver',\n 'simsem',\n 'novel',\n 'aestheticonly',\n 'ptsd',\n 'curriculum',\n 'legs',\n 'php',\n 'pamk',\n 'stattools',\n 'theirs',\n 'meth',\n 'launch',\n 'attentional',\n 'policeman',\n 'leg',\n 'pseudocounts',\n 'nails',\n 'endemic',\n 'parented',\n 'abusing',\n 'hats',\n 'rstanarm',\n 'hospitalisation',\n 'pretest',\n 'converging',\n 'your',\n 'removal',\n 'opencv',\n 'ether',\n 'extremes',\n 'simulator',\n 'know',\n 'dnorm',\n 'gaussian',\n 'kr',\n 'lrts',\n 'fdi',\n 'pixelwise',\n 'vandenberg',\n 'exclusive',\n 'standalone',\n 'preserving',\n 'miserably',\n 'nbom',\n 'persuading',\n 'traindat',\n 'agreeable',\n 'frisch',\n 'arbitrarily',\n 'impairment',\n '---',\n 'recoding',\n 'postscore',\n 'fitbodyfat',\n 'shai',\n 'declare',\n 'appliances',\n 'chollet',\n 'bogged',\n 'horses',\n 'lest',\n 'detrending',\n 'dax',\n 'quantile',\n 'arxiv',\n 'variances',\n 'fulfilled',\n 'digitization',\n 'tabplot',\n 'besides',\n 'undiscovered',\n 'lrtest',\n 'keep',\n 'haskell',\n 'maxes',\n 'digits',\n 'effected',\n 'standards',\n 'limitsellqty',\n 'binomal',\n 'sdev',\n 'services',\n 'wizard',\n 'shoots',\n '--------------------------------------',\n 'modifier',\n 'prespecified',\n 'pythagorean',\n 'way',\n 'modelling',\n 'skyblue',\n 'stayed',\n 'hhh',\n 'vector',\n 'fear',\n 'corroboration',\n 'cst',\n 'absurdly',\n 'datacamp',\n 'obsolete',\n 'lancet',\n 'argentina',\n 'examinations',\n 'modify',\n 'retrospectively',\n 'ppml',\n 'chemical',\n 'malel',\n 'wiggly',\n 'τ',\n 'dnum',\n 'svl',\n 'cocor',\n 'surrogate',\n 'hitting',\n 'promotion',\n 'excessively',\n 'ha',\n 'conditioning',\n 'corner',\n 'ambiguities',\n 'reml',\n 'mertensii',\n 'ratios',\n 'else',\n 'tenfold',\n 'glim',\n 'aid',\n 'erm',\n 'army',\n 'hoenig',\n '✓',\n 'performed',\n 'pandas',\n 'rsquares',\n '-------',\n 'tunning',\n 'logmod',\n 'ntree',\n 'cb',\n 'democrat',\n 'coy',\n 'staggered',\n 'concerned',\n 'synchronization',\n 'given',\n 'ommitted',\n 'stark',\n 'krmodcomp',\n 'unlucky',\n 'reiter',\n 'establishment',\n 'tks',\n 'leafs',\n 'contradiction',\n 'gsp',\n 'golub',\n 'leveled',\n 'sophomore',\n 'krajjhc',\n 'prediction',\n 'laplacesdemon',\n 'lsd',\n 'lacked',\n 'counterexample',\n 'correction',\n 'broadcasting',\n 'linkages',\n 'dur',\n 'randomvariate',\n 'reconstructions',\n 'tweaking',\n 'defintion',\n 'memisc',\n 'noi',\n 'nsamps',\n 'quotients',\n 'subspace',\n 'operators',\n 'nicest',\n 'predictinterval',\n 'bda',\n 'unmanageable',\n 'ruby',\n 'revolve',\n 'sounds',\n 'steven',\n 'reports',\n 'result',\n 'spits',\n 'researcher',\n 'tia',\n 'intrigued',\n 'structural',\n 'reparametrize',\n 'overestimated',\n 'verschuere',\n 'fitc',\n 'bug',\n 'macroeconomic',\n 'fft',\n 'overcomplicating',\n 'xavier',\n 'torso',\n 'stopwords',\n 'toilets',\n 'boole',\n 'foodtype',\n 'ytest',\n 'invert',\n 'mvue',\n 'developing',\n 'mostafa',\n 'asume',\n 'logfreq',\n 'vocab',\n 'goo',\n 'practise',\n 'laerd',\n 'acdf',\n 'boxconstraint',\n 'nnets',\n 'punished',\n 'specialty',\n 'powerfull',\n 'tangle',\n \"\\\\'\",\n 'sham',\n 'dislikes',\n 'estimand',\n 'pathways',\n 'mutatis',\n 'post',\n 'generalises',\n 'withe',\n 'started',\n 'overtime',\n 'cryptographic',\n 'remiss',\n 'humorous',\n 'polson',\n 'aspire',\n 'hinder',\n 'lightregime',\n 'ponder',\n 'bbb',\n 'spectrometry',\n 'unfortunatelly',\n 'omar',\n 'adk',\n 'graft',\n 'handful',\n 'were',\n 'coplot',\n 'contrastive',\n 'marra',\n 'artefacts',\n 'poly',\n 'laypeople',\n 'astronomy',\n 'chd',\n 'tvl',\n 'withhold',\n 'greet',\n 'vegetation',\n 'views',\n 'abstract',\n 'heroic',\n 'zurich',\n 'basetemp',\n 'underly',\n 'extremities',\n 'staph',\n 'sha',\n 'normcdf',\n 'directors',\n 'articles',\n 'dredge',\n 'barycenter',\n '--------------------------------------------------',\n 'dh',\n 'gritty',\n 'écoulé',\n 'negativity',\n 'greenish',\n 'minfunc',\n 'chamberlain',\n 'narrowing',\n 'squareds',\n 'remedied',\n '⟨',\n 'contextually',\n 'medications',\n 'homophily',\n '....?',\n 'wednesdays',\n 'unpooled',\n 'binomials',\n 'seo',\n 'regretfully',\n 'drid',\n '-------------------------------------------------------------------',\n 'pesky',\n 'leverage',\n 'o',\n 'stopping',\n 'interwebs',\n 'somewhere',\n 'folks',\n 'notations',\n 'metastasis',\n 'lengthier',\n 'stochastically',\n 'ϕ',\n 'finaldata',\n 'comprised',\n 'sentences',\n 'trus',\n 'ara',\n 'infinitesimally',\n 'orthogonally',\n 'environmental',\n 'rng',\n 'activity',\n 'standard',\n 'rescue',\n 'suprised',\n 'hawthorne',\n 'generous',\n 'realizing',\n 'pic',\n '----------------------------------------------------------------',\n 'sstotal',\n 'mono',\n 'featuers',\n 'orl',\n 'centimeter',\n 'denoise',\n 'sysevalf',\n 'af',\n 'drained',\n 'scholarship',\n 'dy',\n 'fingerprints',\n 'speech',\n 'ngo',\n 'cervical',\n 'knowlege',\n 'winner',\n 'texting',\n 'soa',\n 'contributions',\n 'disregards',\n 'conflated',\n 'chebychev',\n 'hypersphere',\n 'cdfs',\n 'emphatic',\n 'exaggeration',\n 'perfectionism',\n 'plain',\n 'teacherid',\n 'tsay',\n 'stops',\n 'standarderror',\n 'range',\n 'lmertest',\n 'undergraduate',\n 'respone',\n 'reconciled',\n 'catches',\n 'transects',\n 'untouched',\n 'lac',\n 'recentering',\n 'reminder',\n 'cheng',\n 'mods',\n 'activate',\n 'inject',\n 'la',\n 'closures',\n 'wow',\n 'tstrain',\n 'www',\n 'unilaterally',\n 'sense',\n 'staging',\n 'events',\n 'distribuion',\n 'curvilinearity',\n 'helping',\n 'sitting',\n 'six',\n 'compromise',\n 'bisection',\n 'extras',\n 'coeffs',\n 'piled',\n 'referrals',\n 'eight',\n 'lazarsfeld',\n 'truncates',\n 'gigerenzer',\n 'normally',\n 'whiten',\n 'necessary',\n 'upfront',\n 'menarche',\n 'formula',\n 'inequality',\n 'select',\n 'producing',\n 'audits',\n 'ramp',\n 'lcd',\n 'extensions',\n 'normalisation',\n 'tighten',\n 'audi',\n 'contr',\n 'stderr',\n 'gasram',\n 'chug',\n 'bikes',\n 'benchmarks',\n 'dc',\n 'infinities',\n 'intermediary',\n 'primetype',\n 'aah',\n 'etas',\n 'untangle',\n 'creature',\n 'horowitz',\n 'arts',\n 'flip',\n 'iteratively',\n 'boes',\n 'thorax',\n '∈',\n 'ibm',\n 'guru',\n 'abiotic',\n 'mutiple',\n 'nemenyi',\n 'brr',\n 'chemists',\n 'collinearities',\n 'dgs',\n 'dual',\n 'biol',\n 'meal',\n 'irrational',\n 'sqlite',\n 'submitted',\n 'discretizing',\n 'subsite',\n 'clear',\n 'gav',\n 'valencia',\n 'wines',\n 'nonincreasing',\n 'looping',\n 'pso',\n 'roku',\n ...]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(vocab)","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"29716"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Implement a Trigram Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_model_count = defaultdict(lambda: defaultdict(lambda: 0))\n \nfor text in df_train['tokens']:\n    for w1, w2, w3 in ngrams(text.split(), 3,pad_right=True, pad_left=True,right_pad_symbol='</s>', left_pad_symbol='<s>'):\n        trigram_model_count[(w1, w2)][w3] += 1","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_model_count[('That','is')]['fine']","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_model_count = defaultdict(lambda: defaultdict(lambda: 0))\nfor text in df_train['tokens']:\n    for w1, w2 in ngrams(text.split(),2,pad_right=True, pad_left=True,right_pad_symbol='</s>', left_pad_symbol='<s>'):\n        bigram_model_count[w1][w2] += 1","execution_count":59,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate Log Probability of Sentence\nUsing Additive Laplace smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.01\n# size of vocabulary\nvocab_size = len(set(vocab))\n\ndef get_sentence_prob(sentence,alpha=0.01):\n# use additive Laplace smoothing for calculating probability to handle OOV tokens\n    #seed(10)\n    sent_prob = 0\n    sentence = sentence.split()\n    for token in range(len(sentence)-2):\n        #print(token)\n        #print(trigram_model_count[sentence[token],sentence[token+1]][sentence[token+2]])\n        #print(bigram_model_count[sentence[token]][sentence[token+1]])\n        sent_prob += math.log2((trigram_model_count[sentence[token],sentence[token+1]][sentence[token+2]] + alpha)\n                               /(bigram_model_count[sentence[token]][sentence[token+1]] + alpha*vocab_size))\n    \n    return sent_prob","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_perplexity_score(sentence,alpha):\n    \n    word_count = len(sentence.split()) # Number of words in sentence\n    sent_logprob = get_sentence_prob(sentence,alpha) # Log probability of sentence\n    cross_ent = - sent_logprob / word_count # Cross Entropy\n    perp_score = math.pow(2,cross_ent) # Perplexity\n           \n    return perp_score\n","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Probability and Perplexity for a sentence\nalpha =0.001\nsentence = \"that sounds fine\"\nsent_prob = get_sentence_prob(sentence,alpha)\nsent_perp_score = get_perplexity_score(sentence,alpha)\nprint(\"Probability:{0:.3f}\".format(sent_prob))\nprint(\"perplexity: {0:.3f}\".format(sent_perp_score))\n","execution_count":83,"outputs":[{"output_type":"stream","text":"6\n549\n6\n549\nProbability:-6.592\nperplexity: 4.586\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Compute Perplexity on the entire Training Corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_prob = 0\nalpha = 0.001\ntrigram_cnt = 0\nfor sentence in df_train['tokens']:\n    sentence = sentence.split()\n    sentence =  sentence + ['</s>'] + ['</s>']\n    #print(len(sentence))\n    #print(sentence)\n    for token in range(len(sentence)-2):\n        sent_prob += math.log2((trigram_model_count[sentence[token],sentence[token+1]][sentence[token+2]] + alpha)\n                               /(bigram_model_count[sentence[token]][sentence[token+1]] + alpha*vocab_size))\n        trigram_cnt+=1\n        #print(token)\n        \ncross_ent = -sent_prob / trigram_cnt\nperp_score = math.pow(2,cross_ent)\n\nprint(\"Training set - Perplexity Score: {0:.3f}\".format(perp_score))","execution_count":89,"outputs":[{"output_type":"stream","text":"Training set - Perplexity Score: 37.424\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_corpus_perp_score(datframe, alpha =0.001):\n    sent_prob = 0\n    trigram_cnt = 0\n    for sentence in datframe['tokens']:\n        sentence = sentence.split()\n        sentence =  sentence + ['</s>'] + ['</s>']\n        #print(len(sentence))\n        #print(sentence)\n    for token in range(len(sentence)-2):\n        sent_prob += math.log2((trigram_model_count[sentence[token],sentence[token+1]][sentence[token+2]] + alpha)\n                               /(bigram_model_count[sentence[token]][sentence[token+1]] + alpha*vocab_size))\n        trigram_cnt+=1\n        #print(token)\n        \n    cross_ent = -sent_prob / trigram_cnt\n    perp_score = math.pow(2,cross_ent)\n    \n    return perp_score\n","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.001\nperplexity_score = get_corpus_perp_score(df_train,alpha)\nprint(\"Training set - Perplexity Score: {0:.3f}\".format(perplexity_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.001\nperplexity_score = get_corpus_perp_score(df_test,alpha)\nprint(\"Test set - Perplexity Score: {0:.3f}\".format(perplexity_score))","execution_count":94,"outputs":[{"output_type":"stream","text":"Test set - Perplexity Score: 12.349\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}