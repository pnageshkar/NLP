{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OeIHZdTXg03B"
   },
   "source": [
    "# Task 1: Clean the data\n",
    "The goal of this task to reduce the noise in the original raw text by removing everything that does not bring information to the language model - everything that is not exactly text: html tags, math equations, urls, etc.\n",
    "\n",
    "Finally We want to prepare the corpus and make it ready for our language model by tokenizing the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbiEgK7HS4L4"
   },
   "outputs": [],
   "source": [
    "# We only need the following librairies\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfW0QBdig_k7"
   },
   "source": [
    "Let's load the dataset and shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access files from your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file in to a pandas dataframe\n",
    "data = pd.read_csv('/content/gdrive/My Drive/Data/NLP/stackexchange_812k.csv.gz', \n",
    "                 compression='gzip', header=0, sep=',', quotechar='\"').sample(frac=1,random_state=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 812132 entries, 0 to 812131\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   post_id     812132 non-null  int64  \n",
      " 1   parent_id   75535 non-null   float64\n",
      " 2   comment_id  553076 non-null  float64\n",
      " 3   text        812132 non-null  object \n",
      " 4   category    812132 non-null  object \n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 31.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyWqejm3hUE-"
   },
   "source": [
    "## 2. Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "eiMeKSHbV7Lv",
    "outputId": "bb0bd4bd-6eb9-4c98-f1f7-1a47a8ecdff4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>291254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>601672.0</td>\n",
       "      <td>The condition makes the gradient unbiased. (it...</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>221284.0</td>\n",
       "      <td>Yes, that sounds fine to me.</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>327356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Consider gaussian variables belonging to a ...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>355055.0</td>\n",
       "      <td>Thanks S. Catterall. ^-^ Integrability: I knew...</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>433143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Feature with very few extreme values</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0   291254        NaN    601672.0   \n",
       "1   115372        NaN    221284.0   \n",
       "2   327356        NaN         NaN   \n",
       "3   186923        NaN    355055.0   \n",
       "4   433143        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0  The condition makes the gradient unbiased. (it...  comment  \n",
       "1                       Yes, that sounds fine to me.  comment  \n",
       "2  <p>Consider gaussian variables belonging to a ...     post  \n",
       "3  Thanks S. Catterall. ^-^ Integrability: I knew...  comment  \n",
       "4               Feature with very few extreme values    title  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bon5CFezh_v1"
   },
   "source": [
    "Find out how many categories of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "oQK2WV0JWADk",
    "outputId": "98f300dd-f186-4abb-b45c-ddaac4b59b43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment    553076\n",
       "post       167304\n",
       "title       91752\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the nature of text in the 3 categories - comment, post and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "vR-r4JgDiD5M",
    "outputId": "f08b65d9-05b0-499c-84a9-fafd45aae7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Meta Analysis: Pooling samples or determine an average effect size\n",
      "--------------------------------------------------------------------------------\n",
      "Can multivariate randomness be reduced to one dimensional randomness?\n",
      "--------------------------------------------------------------------------------\n",
      "Large differences between raw (plotted) data and least-squares means from mixed model\n"
     ]
    }
   ],
   "source": [
    "# Examples of titles \n",
    "for txt in data[data.category == 'title'].text.sample(3).values:\n",
    "  print('-' * 80)\n",
    "  print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This appears to be a normal alpha-numeric text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "QagyZuf0iJqO",
    "outputId": "f7ecd5cf-8a19-4f5c-901e-e7a2ff847b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "<p>I am trying to learn EM Algorithm for Gaussian Mixture. But not able to understand few stuffs. This is what I have understood.</p>\n",
      "\n",
      "<p>Consider GMM with k components.\n",
      "$$ p( \\mathbf{x}| \\mathbf{\\alpha_{k}},\\mathbf{\\mu},\\mathbf{\\Sigma})\n",
      " = \\sum_{k=1}^K \\alpha_{k}\\mathcal{N}(\\mathbf{x}|\\mu_{k},\\mathbf{\\Sigma}_{k})$$</p>\n",
      "\n",
      "<p>We have $N$ data points which constitute $\\mathbf{x}$ and we have to find $\\mathbf{\\alpha_{k}},\\mathbf{\\mu},\\mathbf{\\Sigma}$ using EM algo.</p>\n",
      "\n",
      "<p>So we assume some value for $\\mathbf{\\alpha_{k}},\\mathbf{\\mu},\\mathbf{\\Sigma}$ and calculate classification probability $w_{ik}$ for each data point to each Gaussian\n",
      "$$w_{ik} = \\dfrac{ \\alpha_{k}\\mathcal{N}(\\mathbf{x}|\\mu_{k},\\mathbf{\\Sigma}_{k})}{ \\sum_{k=1}^K \\alpha_{k}\\mathcal{N}(\\mathbf{x}|\\mu_{k},\\mathbf{\\Sigma}_{k})}$$\n",
      "So you get a $\\mathbf{W}$ of size $N \\times K$ This is considered as the E step.</p>\n",
      "\n",
      "<p>Now new parameter values are found.\n",
      "$$\\mathbf{\\alpha}_{k}^{new} = \\dfrac{ \\sum_{n=1}^Nw_{ik}}{ \\mathbf{N}}$$\n",
      "$$\\mathbf{\\mu}_{k}^{new} = \\dfrac{ \\sum_{n=1}^Nw_{ik}x_{i}}{ \\sum_{n=1}^Nw_{ik}}$$\n",
      "$$\\mathbf{\\Sigma}_{k}^{new} = \\dfrac{ \\sum_{n=1}^Nw_{ik}(x_{i}-\\mathbf{\\mu}_{k}^{new})(x_{i}-\\mathbf{\\mu}_{k}^{new})^{T}}{ \\sum_{n=1}^Nw_{ik}}$$\n",
      "which is the M step.</p>\n",
      "\n",
      "<ol>\n",
      "<li>How does equations in M step help in finding the maxima of the objective function?  Is $\\prod_{i=1}^{N}\\prod_{k=1}^{K}w_{ik}$ the objective function?</li>\n",
      "<li>I read the explanation for EM algo in <a href=\"http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html\" rel=\"nofollow\">http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html</a>. \n",
      "The article says \"During the E-step, expectation maximization chooses a function $g_{t}$ that lower bounds $log P(x;\\theta)$ everywhere, and for which $g_{t}(\\hat{\\theta} (1))=logP(x;\\hat{\\theta} (t))$. \" I don't understand this.</li>\n",
      "</ol>\n",
      "\n",
      "<p>There is a graphical explanation in <a href=\"http://www.nature.com/nbt/journal/v26/n8/extref/nbt1406-S1.pdf\" rel=\"nofollow\">http://www.nature.com/nbt/journal/v26/n8/extref/nbt1406-S1.pdf</a>. What are the axes of the graph.</p>\n",
      "\n",
      "<ol start=\"3\">\n",
      "<li>What makes this problem to have multiple local maxima rather than than 1 global maxima? that is, when will a problem has a single maxima and when will a problem has multiple local maxima?</li>\n",
      "</ol>\n",
      "\n",
      "<p>I am not able to understand these things. Could some one please explain them?</p>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "<p>In the E-step, why are there 2 different thetas ? Isnt the expectation of a function $ E[x] = \\int xp(x) ?$ If I know x then I would know p(x) as well right ? Based on what I am reading $ E[x] = \\int xp(x') $. Can someone explain to me what is going on ? </p>\n",
      "\n",
      "<p>Also, how do I maximize the expectation of a gaussian function ? Solving the integral gives me the solution, i.e. the mean of the gaussian. I have no variable left like what is doing in the maximization step in the EM algorithm.</p>\n",
      "\n",
      "<p><a href=\"https://i.stack.imgur.com/TVluI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/TVluI.png\" alt=\"enter image description here\"></a></p>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "<p><em>I can give you my own explanation/proof of the PCA, which I think is really simple and elegant, and doesn't require anything except basic knowledge of linear algebra. It came out pretty lengthy, because I wanted to write in simple accessible language.</em></p>\n",
      "\n",
      "<p>Suppose we have some $M$ samples of data from an $n$-dimensional space. Now we want to project this data on a few lines in the $n$-dimensional space, in a way that retains as much variance as possible (that means, the variance of the projected data should be as big compared to the variance of original data as possible).</p>\n",
      "\n",
      "<p>Now, let's observe that if we translate (move) all the points by some vector $\\beta$, the variance will remain the same, since moving all points by $\\beta$ will move their arithmetic mean by $\\beta$ as well, and variance is linearly proportional to $\\sum_{i=1}^M \\|x_i - \\mu\\|^2$. Hence we translate all the points by $-\\mu$, so that their arithmetic mean becomes $0$, for computational comfort. Let's denote the translated points as $x_i' = x_i - \\mu$. Let's also observe, that  the variance can be now expressed simply as $\\sum_{i=1}^M \\|x_i'\\|^2$.</p>\n",
      "\n",
      "<p><strong>Now the choice of the line.</strong> We can describe any line as set of points that satisfy the equation $x = \\alpha v + w$, for some vectors $v,w$. Note that if we move the line by some vector $\\gamma$ orthogonal to $v$, then all the projections on the line will also be moved by $\\gamma$, hence the mean of the projections will be moved by $\\gamma$, hence the variance of the projections will remain unchanged. That means we can move the line parallel to itself, and not change the variance of projections on this line. Again for convenience purposes let's limit ourselves to only the lines passing through the zero point (this means lines described by $x = \\alpha v$).</p>\n",
      "\n",
      "<p>Alright, now suppose we have a vector $v$ that describes the direction of a line that is a possible candidate for the line we search for. We need to calculate variance of the projections on the line $\\alpha v$. What we will need are projection points and their mean. From linear algebra we know that in this simple case the projection of $x_i'$ on $\\alpha v$ is $\\langle x_i, v\\rangle/\\|v\\|_2$. Let's from now on limit ourselves to only unit vectors $v$. That means we can write the length of projection of point $x_i'$ on $v$ simply as $\\langle x_i', v\\rangle$.</p>\n",
      "\n",
      "<p><em>In some of the previous answers someone said that PCA minimizes the sum of squares of distances from the chosen line. We can now see it's true, because sum of squares of projections plus sum of squares of distances from the chosen line is equal to the sum of squares of distances from point $0$. By maximizing the sum of squares of projections, we minimize the sum of squares of distances and vice versa, but this was just a thoughtful digression, back to the proof now.</em></p>\n",
      "\n",
      "<p>As for the mean of the projections, let's observe that $v$ is part of some orthogonal basis of our space, and that if we project our data points on every vector of that basis, their sum will cancel out (it's like that because projecting on the vectors from the basis is like writing the data points in the new orthogonal basis). So the sum of all the projections on the vector $v$ (let's call the sum $S_v$) and the sum of projections on other vectors from the basis (let's call it $S_o$) is 0, because it's the mean of the data points. But $S_v$ is orthogonal to $S_o$! That means $S_o = S_v = 0$.</p>\n",
      "\n",
      "<p><strong>So the mean of our projections is $0$.</strong> Well, that's convenient, because that means the variance is just the sum of squares of lengths of projections, or in symbols $$\\sum_{i=1}^M (x_i' \\cdot v)^2 = \\sum_{i=1}^M v^T \\cdot x_i'^T \\cdot x_i' \\cdot v =  v^T \\cdot (\\sum_{i=1}^M x_i'^T \\cdot x_i) \\cdot v.$$</p>\n",
      "\n",
      "<p><strong>Well well, suddenly the covariance matrix popped out.</strong> Let's denote it simply by $X$. It means we are now looking for a unit vector $v$ that maximizes $v^T \\cdot X \\cdot v$, for some semi-positive definite matrix $X$.</p>\n",
      "\n",
      "<p>Now, let's take the eigenvectors and eigenvalues of matrix $X$, and denote them by $e_1, e_2, \\dots , e_n$ and $\\lambda_1 , \\dots, \\lambda_n$ respectively, such that $\\lambda_1 \\geq \\lambda_2 , \\geq \\lambda_3 \\dots $. If the values $\\lambda$ do not duplicate, eigenvectors form an orthonormal basis. If they do, we choose the eigenvectors in a way that they form an orthonormal basis.</p>\n",
      "\n",
      "<p>Now let's calculate $v^T \\cdot X \\cdot v$ for an eigenvector $e_i$. We have $$e_i^T \\cdot X \\cdot e_i = e_i^T \\cdot (\\lambda_i e_i) = \\lambda_i (\\|e_i\\|_2)^2 = \\lambda_i.$$</p>\n",
      "\n",
      "<p>Pretty good, this gives us $\\lambda_1$ for $e_1$. Now let's take an arbitrary vector $v$. Since eigenvectors form an orthonormal basis, we can write $v = \\sum_{i=1}^n e_i \\langle v, e_i \\rangle$, and we have $\\sum_{i=1}^n \\langle v, e_i \\rangle^2 = 1$. Let's denote $\\beta_i = \\langle v, e_i \\rangle$.</p>\n",
      "\n",
      "<p>Now let's count $v^T \\cdot X \\cdot v$. We rewrite $v$ as a linear combination of $e_i$, and get: $$(\\sum_{i=1}^n \\beta_i e_i)^T \\cdot X \\cdot (\\sum_{i=1}^n \\beta_i e_i) = (\\sum_{i=1}^n \\beta_i e_i) \\cdot (\\sum_{i=1}^n \\lambda_i \\beta_i e_i) = \\sum_{i=1}^n \\lambda_i (\\beta_i)^2 (\\|e_i\\|_2)^2.$$</p>\n",
      "\n",
      "<p>The last equation comes from the fact the eigenvectors where chosen to be pairwise orthogonal, so their dot products are zero. Now, because all eigenvectors are also of unit length, we can write $v^T \\cdot X \\cdot v = \\sum_{i=1}^n \\lambda_i \\beta_i^2$, where $\\beta_i ^2$ are all positive, and sum to $1$.</p>\n",
      "\n",
      "<p><strong>That means that the variance of the projection is a weighted mean of eigenvalues. Certainly, it is always less then the biggest eigenvalue, which is why it should be our choice of the first PCA vector.</strong></p>\n",
      "\n",
      "<p>Now suppose we want another vector. We should chose it from the space orthogonal to the already chosen one, that means the subspace $\\mathrm{lin}(e_2, e_3, \\dots , e_n)$. By analogical inference we arrive at the conclusion, that the best vector to project on is $e_2$. And so on, and so on...</p>\n",
      "\n",
      "<p>By the way, it should be now clear, why the variance retained can be expressed by $\\sum_{i=1}^k \\lambda_i / \\sum_{i=1}^n \\lambda_i$.</p>\n",
      "\n",
      "<p><strong>We should also justify the greedy choice of vectors.</strong> When we want to choose $k$ vectors to project onto, it might not be the best idea to first choose the best vector, then the best from what remains and so on. I'd like to argue that in this case it is justified and makes no difference. Lets denote the $k$ vector we wish to project onto by $v_1, \\dots , v_k$. Also, let's assume the vectors are pairwise orthogonal. As we already know, the total variance of the projections on those vectors can be expressed by $$\\sum_{j=1}^k \\sum_{i=1}^n \\lambda_i \\beta_{ij}^2 = \\sum_{i=1}^n \\lambda_i \\gamma_i$$ where $\\gamma_i = \\sum_{j=1}^k \\beta_{ij}^2.$</p>\n",
      "\n",
      "<p>Now, let's write $e_i$ in some orthonormal basis that includes $v_1, \\dots , v_k$. Let's denote the rest of the basis as $u_1, \\dots, u_{n-k}$. We can see that $e_i = \\sum_{j=1}^k \\beta_{ij} v_j + \\sum_{j=1}^{n-k} \\theta_j \\langle e_i, u_j \\rangle$.\n",
      "Because $\\|e_i\\|_2 = 1$, we have $\\sum_{j=1}^k \\beta_{ij}^2 + \\sum_{j=1}^{n-k} \\theta_j^2 = 1$, and hence $\\gamma_i \\leq 1$ for all $i$.</p>\n",
      "\n",
      "<p>Now we have a similar case to one vector only, we now know that the total variance of projections is $\\sum_{i=1}^n \\lambda_i \\gamma_i$ with $\\gamma_i \\leq 1$ and $\\sum_{i=1}^n \\gamma_i = k$. This is yet another weighted mean, and is certainly no more than $\\sum_{i=1}^k \\lambda_i$ which corresponds to projecting on $k$ eigenvectors corresponding to biggest eigenvalues.</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examples of posts\n",
    "for txt in data[data.category == 'post'].text.sample(3).values:\n",
    "  print('-' * 80)\n",
    "  print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that text in post have html tags and latex formatted equations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "No7lYssNiNCy",
    "outputId": "13568e3d-b64c-4d1f-904d-be913cbf2b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Take a look at the circular package. It has several plots that may help you, and also some nice functions.\n",
      "--------------------------------------------------------------------------------\n",
      "@GermanDemidov I disagree with your points here. One can easily calibrate the T-test when the distribution is small and non-normal, even multimodal (use simulation). It's easy to show the type-1 error rate is conserved and that the test has some power. At any rate, a jackknife could be recommended as an alternative. The important point is coding LLD values as 0 when they are positively valued.\n",
      "--------------------------------------------------------------------------------\n",
      "@TenaliRaman That is not true. I studied on Casella Berger and on chapter 4, par 4.4, page 162-165 they explicitly talks about Hierarchical Model and Mixture distribution. \" .. $X$ is said to have a mixture distribution if the distribution of $X$ depends on a quantity that also has a distribution.\" Following this argument I correclty described a mixture of gaussians, unless you want to contradict one of the most used statistics textbooks in the entire world. Please remove the downvote.\n"
     ]
    }
   ],
   "source": [
    "# And here's a sample of comments\n",
    "for txt in data[data.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 80)\n",
    "  print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While most of this is normal text - some mentions (@username) are visible**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mk-vHb7LiZrf"
   },
   "source": [
    "## 3. Clean up raw text\n",
    "We're going to remove the following elements:\n",
    "* html tags\n",
    "* line returns\n",
    "* urls\n",
    "* latex equations\n",
    "* numbers\n",
    "* mentions: @someone\n",
    "* digits\n",
    "* most of the punctuation\n",
    "* and extra spaces\n",
    "\n",
    "For that we will use a series of simple regex patterns and the following pandas dataframe pattern:\n",
    "\n",
    "```\n",
    "pattern = r\" some regex pattern\"\n",
    "df.text.apply(lambda t : re.sub(pattern,' ', txt) )\n",
    "```\n",
    "\n",
    "\n",
    "Note that the regex patterns used here are chosen for their simplicity. Feel free to use more precise patterns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAr8qzoKiQjx"
   },
   "outputs": [],
   "source": [
    "# remove html tags\n",
    "data['text'] = data.text.apply(lambda txt : re.sub(r\"<[^>]*>\",' ', txt) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DS_VHPC1jN5i"
   },
   "outputs": [],
   "source": [
    "# remove line returns\n",
    "data['text'] = data.text.apply(lambda txt : re.sub(r\"[\\r\\n]+\",' ', txt) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyQeTFN9jPvK"
   },
   "outputs": [],
   "source": [
    "# remove urls\n",
    "data['text'] = data.text.apply(lambda txt : re.sub(r'http.?://[^\\s]+[\\s]?', ' ', txt) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TI1vZWUFjRCT"
   },
   "outputs": [],
   "source": [
    "# remove mentions\n",
    "data['text'] = data.text.apply(lambda txt : re.sub(r\"@\\S+\",' ', txt) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "29nqngCpjST2"
   },
   "outputs": [],
   "source": [
    "# remove latex\n",
    "data['text'] = data.text.apply(lambda txt : re.sub(r\"\\$[^>]*\\$\",' ', txt) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2ZBG7PkjTlQ"
   },
   "outputs": [],
   "source": [
    "# remove digits\n",
    "data['text'] = data.text.apply(lambda txt : re.sub(r\"\\d+\",' ', txt) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEmEZVOwjVmy"
   },
   "outputs": [],
   "source": [
    "# remove some of the punctuation but keep ,.!? and -\n",
    "remove = '\"#$%&()*+/:;<=>@[\\\\]^_`{|}~”“'\n",
    "pattern = r\"[{}]\".format(remove)\n",
    "data['text'] = data.text.apply(lambda txt : re.sub(pattern,' ', txt) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhtXc_dPjX88"
   },
   "outputs": [],
   "source": [
    "# remove multiple spaces\n",
    "data['text'] = data.text.apply(lambda txt : re.sub(r\"\\s\\s+\",' ', txt) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYJyjInajZQG"
   },
   "outputs": [],
   "source": [
    "# finally remove trailing spaces with strip()\n",
    "data['text'] = data.text.apply(lambda txt : txt.strip() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nY6RQnqSjbk3"
   },
   "source": [
    "Let's check out the resulting text for the different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "j7D5HfhXjZ8t",
    "outputId": "77b24bea-2d06-460e-dd13-ed9f66229658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "How to calculate 'times likely' for a percentage and whole number data set?\n",
      "--------------------------------------------------------------------------------\n",
      "Logistic regression with an independent variable that only applies to a subset\n",
      "--------------------------------------------------------------------------------\n",
      "Why we always put log before the joint pdf when we use MLE Maximum likelihood Estimation ?\n"
     ]
    }
   ],
   "source": [
    "# titles should not be changed\n",
    "for txt in data[data.category == 'title'].text.sample(3).values:\n",
    "  print('-' * 80)\n",
    "  print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "_pyo6Bdhjl4J",
    "outputId": "4a185842-a6d7-4a3c-eaa0-51f86a5966f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "A little late but still can help some. I tested on almost the same sample as you, I just tested with only selected variable max depth library h o data iris h o.init iris Species We obtain a distribution of each node other virginica Then the relative importance from h o is based on the following calculation \\begin align RelativeImportance amp \\frac - \\frac \\frac amp . \\end align Same result with H O h o.varimp rf Variable Importances variable relative importance scaled importance percentage Petal.Width . . . Sepal.Length . . . Sepal.Width . . . Petal.Length . . . The relative importance of a variable as a general is certainly the sum of the previous function applied on each node where the variable is used. I don't know how to interpret the function used, if someone can, it will be with pleasure. Hope it helped!\n",
      "--------------------------------------------------------------------------------\n",
      "For a data set with replicates of different treatments samples what would be the best way to run the cross validation of PLSR models? There are not enough samples to have separate training and test sets. My initial thought was to just do k-fold cross validation with randomly generated segments. However after reading some of the information from Camo in the Unscrambler help menu they suggest that all segments should contain unique information . Therefore should I use systematic segmentation and have segments each with all replicates of one treatment? Your opinions would be appreciated.\n",
      "--------------------------------------------------------------------------------\n",
      "I'm not sure I understand your question exactly, but I assume you are looking for the probability mass at each point, where an event is defined as a subsegment covering a particular position. If this is true, I believe you should be able to work out the exact probability mass function. For each subsegment of length K the distribution at a particular location between K and N-K is uniform and proportional to K. The distribution at the tails is stepwise increasing from to K. This can be seen by just working out one example. Given multiple subsegments of different sizes, simply add these functions up. You can then normalize everything so the weights sum to one if you want a proper distribution function. Given a length and a list of subsegment lengths, this probability mass function should be easy to code up in the language of your choice. If you are interested in the number of segments covering any point, this is simply the sum of Bernoulli random variables with different p, defined at each point using the same pmf. The simplest approach would be to enumerate the possibilities from to K.\n"
     ]
    }
   ],
   "source": [
    "# posts should have much less clutter\n",
    "for txt in data[data.category == 'post'].text.sample(3).values:\n",
    "  print('-' * 80)\n",
    "  print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "KKeBjpmsjxjZ",
    "outputId": "7301b185-7987-4503-8f63-20a1eb715962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "I took a quick glance at the GSL documentation and based on what it offers I would presume it cannot do anything like what you ask. It may be worth remarking that what you are attempting is unusual and might be inferior to other procedures. Perhaps you might like to consider modifying your post describe your data and what you would like to learn from them, then let readers suggest appropriate procedures.\n",
      "--------------------------------------------------------------------------------\n",
      "I don't get what you mean by companion functions. The problem can be solved by inverting the transformations, finding the joint support of your new random variables and multiplying by the Jacobian.\n",
      "--------------------------------------------------------------------------------\n",
      "I think that in ANOVA-type interaction diagrams the estimated marginal means would be plotted. These are equivalent to the means of the predicted conditional means. Not sure if this helps.\n"
     ]
    }
   ],
   "source": [
    "# comments should also be less noisy\n",
    "for txt in data[data.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 80)\n",
    "  print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4T1CQqWk6rP"
   },
   "source": [
    "## 4. Tokenize\n",
    "\n",
    "Let's tokenize the text. \n",
    "This will allow us to count the number of tokens of each text and subsequently remove test that are too long or too short.\n",
    "You can use other librairies to tokenize the text (spacy for instance) or other tokenizer. Here we use the [WordPunctTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.WordPunctTokenizer) from NLTK.\n",
    "\n",
    "And we create a new columns called tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ge5LTc3j4Te"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "data['tokens'] = data.text.apply(lambda t : tokenizer.tokenize(t.lower())) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ll09OAjmGby"
   },
   "source": [
    "Let's now count the tokens in each piece of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3dp7L-Hl-AR"
   },
   "outputs": [],
   "source": [
    "data['n_tokens'] = data.tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "dzIE6rdWmPCo",
    "outputId": "40670ac8-d6c1-4909-e7fa-128630266d34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    812132.000000\n",
       "mean         60.074291\n",
       "std          99.416095\n",
       "min           0.000000\n",
       "25%          16.000000\n",
       "50%          35.000000\n",
       "75%          70.000000\n",
       "max       10874.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.n_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "PrwoRVQnmRa3",
    "outputId": "9b208aa6-0e3b-48d7-e15f-417e4b6536ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.n_tokens.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJGNrR5wmV5r"
   },
   "source": [
    "**We notice that there are some extremely long texts. Let's look at the largest one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "SgsxoSdWmUe3",
    "outputId": "6a72f8d1-d2ac-45b7-b1c9-0e669b025187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sample includes subjects, of which belong to group L , while the other to group L please see data below . I used GLM for a binary outcome to test for group differences in background variables - summary pre lt - glm L g a m p e, family binomial logit , data df ...yielding significant differences for of them g, a, m, p, and e . So I modeled these background variables as covariates when testing for an association between my predictor, chr and my outcome rsk , in each one of the groups L , L , again using GLM for binary outcome summary fit lt - glm rsk chr g a m p e, family binomial logit , data df which df L , The results showed that a significant association does exist for L but not for L . I would appreciate your help in how to test whether significance non-significance can be attributed to the group condition? . Or in other words, is it true that for subjects L , a significant correlation is evident, while for L ' it's absent. Thanks for responders! Uri structure list L c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , g c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , a c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , m c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , p c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , e c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , chr c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , rsk c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , row.names c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , class data.frame\n"
     ]
    }
   ],
   "source": [
    "# this one has a very long series of \"L,\"\n",
    "print(data[data.n_tokens > 10000].text.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nK4wiFJamvac"
   },
   "source": [
    "We can see that most of the longest texts are composed of tables with limited semantic value. \n",
    "We will remove rows that have more than an arbitrary number of tokens (let's say 5000) as well as rows that have too few tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cw8QXJ9CmjTo",
    "outputId": "5466e0dd-77c2-45ea-bd71-796063abbbe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789649, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[(data.n_tokens > 4) & (data.n_tokens < 5000)].reset_index(drop = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "P2IOaPeKsDHO",
    "outputId": "7a2883b6-6a39-48ac-df35-fec8290ebe4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment    540587\n",
       "post       165377\n",
       "title       83685\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fe4WDsdFnQ29"
   },
   "source": [
    "## 5. Export data\n",
    "We could export the dataframe as such using a pickle file format. \n",
    "\n",
    "However if we want to keep the original csv format it's going to be easier if we transform the list of tokens into a space separated string.\n",
    "\n",
    "On retrieval we will only have to split the string to get back the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "vgqfut5snKj4",
    "outputId": "91673f01-cb41-45ee-f755-60a15efb3ade"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the condition makes the gradient unbiased . it...\n",
       "1                       yes , that sounds fine to me .\n",
       "2    consider gaussian variables belonging to a gau...\n",
       "3    thanks s . catterall . - integrability i knew ...\n",
       "4                 feature with very few extreme values\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda tk : ' '.join(tk))\n",
    "data.tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wfr2MyKoAtn"
   },
   "source": [
    "And finally let's export the dataframe into a csv file.\n",
    "We will use that csv file as the new cleaned up and filtered out dataset to build our language model in task 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtMO9S-Jn9Nf"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/gdrive/My Drive/Data/NLP//stackexchange_812k_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-cb208fff6516>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive/My Drive/Data/NLP//stackexchange_812k_v2.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQUOTE_ALL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\anaconda3\\envs\\aamlp\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\aamlp\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             )\n\u001b[0;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\aamlp\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/Data/NLP//stackexchange_812k_v2.csv'"
     ]
    }
   ],
   "source": [
    "data.to_csv('/content/gdrive/My Drive/Data/NLP//stackexchange_812k_v2.csv', quoting = csv.QUOTE_ALL, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6VZYBr8oYEY"
   },
   "source": [
    "# Conclusion\n",
    "Removing or adding steps to this first text processing task will allow us to test different approaches in our language model building process.\n",
    "\n",
    "For instance we can decide not to remove the latex formatted mathematical equation and see if the language model is able to create grammatically valid equations. \n",
    "\n",
    "We could also implement a step to handle contractions (i'm, let's, ...) and see if that improves the quality of the generated text\n",
    "\n",
    "Finally we could also decide to work on the vocabulary and filter out typos or non-English unknown words using named entity recognition to tag specific tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgEZZVcjoU6Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LPLMDL Task 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
